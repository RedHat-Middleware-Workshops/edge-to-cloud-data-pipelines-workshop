:imagesdir: ../assets/images

== Investigating and Testing Edge Services

== Overview
The previous section introduced the main application components making up the composite *Shopping Application*.

In this section you will:

* Learn about how AI/ML models can be deployed or _operationalized_ on the edge.
* Take a deeper look at the Model Server and how it can run an inference against an existing deployed model.
* Test the Pricing Engine by invoking it directly.

{empty} +

NOTE: In the next section we will cover the *shopper* service which provides the shoppers GUI experience and invokes the above services and their dependencies.

You should be logged into the OpenShift Console, if you timed out or logged out, follow these instructions.  

xref:includes/01-ocp-re-open-console.adoc[Log Back into OpenShift,role=resource,window=_blank]

{empty} +

== Operationalizing Models onto The Edge
This section and the next section focus on services running in the Edge environment, and how they interact with deployed AI/ML models.

_In a later section in this lab you will walk through adding a new product to your "store" on the edge.  The whole process or "MLOps Pipeline" of moving new dataset images to a centralized data science environment to build a new model, training the new model, testing it, and pushing it out to the edge will be covered._

This section focuses on the edge experience using models. It is important to recognize our use case, which is one where store/kiosks are edge based in that they have their own remote compute capability in the store, sometimes called the "Near Edge."

The customers and store associates use the shopping application (running on the Near Edge in the store) from their edge devices such as phones, Point of Sale devices, or computers.

{empty} +

== The Model Server *tf-server*

You already had a brief experience with the Model Server in the MlOps (lab 1). Here you will gain a deeper perspective on how it operates on the edge and within an OpenShift environment.

There are many different model building and model serving engines and topologies that an Edge architecture can use.  Red Hat OpenShift AI supports using commerical 3rd party approaches, and also allows developers and data scientists to use upstream open source approaches.

Our use case supports stores which may not have constant connectivity to a centralized model server, and may also have very custom product models deployed for their store's niche clientele.  

For this Art of the Possible approach the lab developers chose to use a flexible _model serving_ approach deploying *TensorFlow Serving* engine in the edge environment.  This allows the Shopping Application to function in a disconnected fashion from the central model training and model repository.  

The tensorflow approach also allows for using models that are setup to work with Base64 image formats which eases use. The model server is invoked via sending an image through a REST call.

=== Steps

* Start in Developer view of your {user}-lab2-edge project
** This may appear a bit cluttered as supporting services are also represented.
+
[.bordershadow]
image::02-02/01-edge-prj-main-pieces.png[width=75%]

{empty} +

* Locate the tf-server deployment
** You may have to click on the screen and hold/drag the mouse to locate it, or use the directional tools at the bottom of the screen.
** The tf-server deployment is a pre-built tensorflow engine packaged as a container image. You can learn more about TensorFlow from the TensorFlow site https://www.tensorflow.org/[TensorFlow Site,role=resource,window=_blank]

{empty} +

* Click on the *tf-server* deployment and look at the deployment configuration
** Click on the tf-server deployment icon
+
[.bordershadow]
image::02-02/01-tf-server.png[width=25%]

{empty} +

** This will open the deployment pane to the right, click on the *Actions* dropdown box indicator
+
[.bordershadow]
image::02-02/02-tf-server.png[width=75%]

{empty} +

** Scroll down the listing and click on *Edit Deployment*
+
[.bordershadow]
image::02-02/03-tf-server.png[width=75%]

{empty} +

** This will open the OpenShift Deployment editor
+
[.bordershadow]
image::02-02/04-tf-server.png[width=75%]

{empty} +

** Scroll down to the *Environment Variables* Section
+
[.bordershadow]
image::02-02/05-tf-server.png[width=75%]
+
* Notice that in the OpenShift deployment of the TensorFlow Server we just need to set a few variables.  The model server (TensorFlow) just needs to know where the current models are located and which model should be picked.  Later when you train a new model the model server will automaticaly use the new model version deployed to the edge.
+
In many scenarios you would put these variables into an OpenShift ConfigMap, and externalize them from the deployment manifest. We will cover that approach with the pricing server.

{empty} +

* Close the deployment editor by clicking on *Cancel*
+
[.bordershadow]
image::02-02/06-tf-server.png[width=75%]

{empty} +

== Invoking the Pricing Engine:  *price-engine*

In this Art of the Possible Lab, the business logic to decide a price is implemented by a simple micro service named *price-engine*.  

_When building edge applications consisting of multiple services.  You can quickly create small micro services in OpenShift to test functionality for different scenarios._

The Pricing Engine is a Red Hat Camel K micro service that quickly loads up a product pricing list that is written in json.

The json file is actually deployed into OpenShift as a file tied to a ConfigMap that is associated with the *price-engine*.

* First take a quick look at the ConfigMap *catalogue* that the Pricing Engine loads up and checks when it is invoked.

* Click on ConfigMaps, then click on *catalogue*
+
[.bordershadow]
image::02-02/01-03-catalogue.png[width=75%]

{empty} +

* Scroll down to the Data section. You can work more directly with this data and approach in the optional last section in this module.
+
[.bordershadow]
image::02-02/01-03.1-catalogue.png[width=75%]
+
[.bordershadow]
image::02-02/01-03.2-catalogue.png[width=75%]
+
NOTE:  The *shopper* service calls the *pricing-engine* and passes it the "item: <tea name>".  The microserivce then looks up and returns the label and price, if it is in the catalogue listing.

{empty} +

TIP: _There is an optional section at the end of the lab that takes you into the Camel K approach.  It uses DevSpaces and gives a deeper developer view of the possibilites of expanded use of application services and inner loop development on OpenShift._

The following steps allow you to directly invoke the Pricing Engine.

{empty} +

=== Steps

* You should be in the topology pane of the developer view in the OpenShift Console.
+
[.bordershadow]
image::02-02/01-edge-prj-main-pieces.png[width=75%]

{empty} +

* Locate the *price-engine* deployment
** You may have to click on the screen and hold/drag the mouse to locate it, or use the directional tools at the bottom of the screen.
+
[.bordershadow]
image::02-02/01-price-engine.png[width=20%]

{empty} +

* Click on the *price-engine* deployment
* When the Deployment screen appears, click on Details
+
[.bordershadow]
image::02-02/02-price-engine.png[width=75%]

{empty} +

* Scroll down till you see the Pod icon
+
[.bordershadow]
image::02-02/04-price-engine.png[width=75%]
+
Notice that if you were carrying out some scalability testing on the pricing service you could quickly increase the pod count. You could also setup an auto-pod scaler configuration.  This type of functionality might be important for large surges of customers.

{empty} +

* Click on the *Resources* tab
+
[.bordershadow]
image::02-02/05-price-engine.png[width=75%]

{empty} +

* Scroll down till you get the exposed Route
+
[.bordershadow]
image::02-02/06-price-engine.png[width=75%]
+
The Route exposes the Pricing Engine outside of our OpenShift Cluster. In many real-world scenarios the pricing services may actually be in a different cluster and not remotely installed with the Shopping Application.  In those cases you can still call the Pricing Engine through its exposed route.

{empty} +

We will simulate that now by calling the pricing engine directly using it's exposed route URL instead of it's cluster limited service definition.

*OpenShift Web Terminal*: Inside OpenShift there is an ability to open a terminal window, which runs a container on the cluster, and allows users to invoke command line instructions.

include::partial$ocp-open-cmd-line-terminal.adoc[]


* Now you can test the pricing engine directly by running a Curl command in the terminal window.
+
[.console-input]
[source,adoc]
[subs=attributes+]
curl \
-H "item: tea-lemon" \
http://price-engine-{user}-lab2-edge.{openshift_cluster_ingress_domain}/price

* You should see output such as the following.
+
[.bordershadow]
image::02-02/07-price-engine.png[]

* You can now shutdown the Web Terminal Window by clicking on the *X* in the upper right-hand corner of the window
+
[.bordershadow]
image::02-02/08-close-web-terminal.png[]
* And clicking *Yes* at the following confirmation MessageBox
+
[.bordershadow]
image::02-02/09-confirm-terminal-close.png[width=50%]

{empty} +

Now that you have been able to test the functionality of two key parts of our Edge application, Let's work with the *shopper* application that calls these services.







